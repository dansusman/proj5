#!/usr/bin/python3
#
# CS3700, Fall 2021
# Dean Frame, Dan Susman
# Web Crawler, Project 5

import re
import socket
import ssl
import sys
import time
from html.parser import HTMLParser
from typing import Dict, Tuple

URL = "fakebook.3700.network"
PORT = 443
BUFSIZE = 4096

context = ssl.create_default_context()
sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
s_sock = context.wrap_socket(sock, server_hostname=URL)
s_sock.connect((URL, PORT))


class Crawler:
    username = None
    password = None
    cookie = None
    token = None
    session = None
    pages_to_visit = set()  # deque of page urls that we need to explore
    seen_pages = set()  # set of page urls that we have already explored

    def set_username(self, username):
        """Sets the username of the crawler to the given string."""
        self.username = username

    def set_password(self, password):
        """Sets the password of the crawler to the given string."""
        self.password = password

    def set_cookie(self, cookie):
        """Sets the current cookie (anti-CSRF token) of the
        crawler to the given string.
        """
        self.cookie = cookie

    def set_token(self, token):
        """Sets the CSRF middleware token to the given string."""
        self.token = token

    def set_session(self, sess_id):
        """Sets the session ID to the given string."""
        self.session = sess_id

    def add_urls_to_deque(self, urls):
        """Adds the given list of found urls to the current
        deque of page URLs to visit."""
        self.pages_to_visit.update(urls)

    def get_token_and_cookie(self) -> Tuple[str, str]:
        """Gets the CSRF middleware token and web app cookie."""
        global url
        method = "GET"
        version = "HTTP/1.1"
        host = "Host: fakebook.3700.network"
        accept = "Accept: text/html, application/xhtml+xml"
        connnection = "Connection: keep-alive"
        user_agent = "User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36"
        request = method + " " + "/accounts/login/?next=/fakebook/" + " " + version + "\r\n" + \
            host + "\r\n" + connnection + "\r\n" + accept + "\r\n" + user_agent + "\r\n\r\n"
        s_sock.send(request.encode())
        response = s_sock.recv(BUFSIZE).decode()
        return self.parse_for_cookie_and_token(response)

    def parse_for_cookie_and_token(self, html_and_head) -> Tuple[str, str]:
        """Parses the given string (HTML and Headers) for the cookie and
        CSRF middleware token."""
        status_code, headers, html = self.separate(html_and_head)
        token_idx = html.index("csrfmiddlewaretoken")
        token = html[token_idx+28:token_idx + 92]
        cookie = headers["Set-Cookie"]
        cookie = cookie[:cookie.index(";")]
        self.set_cookie(cookie)
        self.set_token(token)
        return cookie, token

    def get_status(self, html_and_head):
        """Parses for the status code of the given response string."""
        status_code = re.search("(\d\d\d)", html_and_head)
        if status_code:
            return status_code.group()
        else:
            print("Invalid Response:", html_and_head)
            return "500"

    def separate(self, html_and_head) -> Tuple[str, Dict[str, str], str]:
        """Separates the Headers from the HTML."""
        status_code = self.get_status(html_and_head)
        doc_idx = html_and_head.index("<!DOCTYPE")
        header = html_and_head[html_and_head.index("\r\n"):doc_idx]
        header = header.replace("\n ", " ").splitlines()
        html = html_and_head[doc_idx:]
        headers = {}
        for line in header:
            split_here = line.find(":")
            headers[line[:split_here]] = line[split_here+2:]
        return status_code, headers, html

    def post_login(self):
        """Makes a POST call to log in the user with the
        credentials given at the command line.
        """
        method = "POST"
        version = "HTTP/1.1"
        host = "Host: fakebook.3700.network"
        accept = "Accept: text/html, application/xhtml+xml"
        connnection = "Connection: keep-alive"
        cookie = "Cookie: " + self.cookie
        user_agent = "User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36"
        content_type = "Content-Type: application/x-www-form-urlencoded"
        payload = "username=" + self.username + "&password=" + self.password + \
            "&csrfmiddlewaretoken=" + self.token + "&next=%2Ffakebook%2F"
        content_length = "Content-Length: " + str(len(payload))
        request = method + \
            " " + \
            "/accounts/login/?next=/fakebook/" + \
            " " + version + \
            "\r\n" + host + \
            "\r\n" + content_type + \
            "\r\n" + connnection + \
            "\r\n" + cookie + \
            "\r\n" + accept + \
            "\r\n" + content_length + \
            "\r\n\r\n" + payload
        s_sock.send(request.encode())
        response = s_sock.recv(BUFSIZE).decode()
        status = self.get_status(response)
        if (status == "302"):
            location = "/fakebook/"
            self.parse_for_cookie(response)
            self.parse_for_session(response)
            return self.http_get(location)

    def parse_for_cookie(self, response):
        """Parses the given response string for a session ID."""
        idx = response.index("csrftoken=")
        cookie = response[idx:]
        cookie = cookie[:cookie.index(";")]
        self.set_cookie(cookie)

    def parse_for_session(self, response):
        """Parses the given response string for a session ID."""
        idx = response.index("sessionid=")
        session = response[idx:]
        session = session[:session.index(";")]
        self.set_session(session)

    def http_get(self, url) -> Tuple[str, str]:
        """Performs an HTTP GET Request and returns the response
        from Fakebook's server.
        """
        method = "GET"
        version = "HTTP/1.1"
        host = "Host: fakebook.3700.network"
        accept = "Accept: text/html, application/xhtml+xml"
        connnection = "Connection: keep-alive"
        cookie = "Cookie: " + self.cookie + "; " + self.session
        request = method + " " + url + " " + version + \
            "\r\n" + host + \
            "\r\n" + connnection + \
            "\r\n" + cookie + \
            "\r\n" + accept + "\r\n\r\n"
        s_sock.send(request.encode())
        response = s_sock.recv(BUFSIZE).decode()
        if "Set-Cookie: csrftoken=" in response:
            self.parse_for_cookie(response)
        if "Set-Cookie: sessionid=" in response:
            self.parse_for_session(response)
        return response, url

    def search_next_page(self):
        """Searchs the next page in the deque."""
        url = self.pages_to_visit.pop()
        return self.http_get(url)


class AnchorParser(HTMLParser):
    """Class to parse HTML for <a> tags."""
    found_urls = []

    def handle_starttag(self, tag, attrs):
        if tag == "a":
            for attr in attrs:
                if attr[0] == "href":
                    self.found_urls.append(attr[1])


crawler = Crawler()
parser = AnchorParser()
if len(sys.argv) != 3:
    print(
        "Please use this module by calling ./webcrawler [username] [password]!")
else:
    crawler.set_username(sys.argv[1])
    crawler.set_password(sys.argv[2])

# scrape for cookie, set to crawler
crawler.get_token_and_cookie()
response, url = crawler.post_login()

# data structure to hold secret flags
secret_flags = []
while len(secret_flags) < 5:
    # get the status code of the response
    if "Connection: close" in response or len(response) == 0:
        time.sleep(2)
        # kill socket, start again with new socket
        s_sock.close()
        sock.close()
        context = ssl.create_default_context()
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        s_sock = context.wrap_socket(sock, server_hostname=URL)
        s_sock.connect((URL, PORT))
        response, url = crawler.http_get(url)

    status_code = crawler.get_status(response)
    if status_code == "403" or status_code == "404":
        # abandon the current url --> move to the next url in the crawler's deque
        print(status_code, url)
        response, url = crawler.search_next_page()
    elif status_code == "500":
        # try the current request again.
        response, url = crawler.http_get(url)
    elif status_code == "302":
        location = response[response.index("Location:")+10:]
        location = response[:response.index("\\r\\n")]
        location = location[:-4]
        response, url = crawler.http_get(location)
    elif status_code == "200":
        crawler.seen_pages.add(url)
        # look for secret flag
        searchResult = re.findall(
            "secret_flag.+FLAG: ([a-zA-Z0-9]{64})", response)
        if searchResult:
            secret_flags.extend(searchResult)
            print(searchResult[0])

        # look for anchors
        parser.feed(response)
        new_urls = parser.found_urls
        filtered_urls = list(filter(lambda possible_url: possible_url not in crawler.seen_pages
                                    and "logout" not in possible_url
                                    and "admin" not in possible_url, new_urls))

        crawler.add_urls_to_deque(filtered_urls)
        parser.found_urls = []

        response, url = crawler.search_next_page()
    else:
        print(url)
        print(response)
        print("Unknown status code:", status_code)
        sys.exit(0)

# Chunking? HTTP1.0 vs HTTP1.1
        # if response closing tag != </html>, continue reading from sock


# only in the case that we don't see "/html>\n"
# read more....
# Nevermind to the below comments
# read only header to grab Content-Length
# read Content-Length worth of bytes, if "</html>\n" not in bytes read, read more
