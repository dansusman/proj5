#!/usr/bin/python3
#
# CS3700, Fall 2021
# Dean Frame, Dan Susman
# Web Crawler, Project 5

import socket
import ssl
import sys
from typing import Dict, Tuple
from collections import deque
from html.parser import HTMLParser
import re

URL = "fakebook.3700.network"
PORT = 443
BUFSIZE = 4096

context = ssl.create_default_context()
sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
s_sock = context.wrap_socket(sock, server_hostname=URL)
s_sock.connect((URL, PORT))

class Crawler:
    username = None
    password = None
    cookie = None
    token = None
    session = None
    pages_to_visit = deque() # deque of page urls that we need to explore
    seen_pages = set() # set of page urls that we have already explored

    def __init__(self):
        """TODO: Add docstring"""

    def set_username(self, username):
        """TODO:"""
        self.username = username
        
    def set_password(self, password):
        """TODO:"""
        self.password = password

    def set_cookie(self, cookie):
        """TODO:"""
        self.cookie = cookie

    def set_token(self, token):
        """TODO:"""
        self.token = token

    def set_session(self, sess_id):
        """TODO:"""
        self.session = sess_id

    def add_urls_to_deque(self, urls):
        self.pages_to_visit.extend(urls)

    def get_token_and_cookie(self) -> Tuple[str, str]:
        """TODO:"""
        global url
        method = "GET"
        version = "HTTP/1.1"
        host = "Host: fakebook.3700.network"
        accept = "Accept: text/html, application/xhtml+xml"
        connnection = "Connection: keep-alive"
        user_agent = "User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36"
        request = method + " " + "/accounts/login/?next=/fakebook/" + " " + version + "\r\n" + host + "\r\n" + connnection + "\r\n" + accept +  "\r\n" + user_agent + "\r\n\r\n"
        s_sock.send(request.encode())
        response = s_sock.recv(BUFSIZE).decode()
        # print(response)
        return self.parse_for_cookie_and_token(response)
    
    def parse_for_cookie_and_token(self, html_and_head) -> Tuple[str, str]:
        """TODO:"""
        status_code, headers, html = self.separate(html_and_head)
        token_idx = html.index("csrfmiddlewaretoken")
        token = html[token_idx+28:token_idx + 92]
        cookie = headers["Set-Cookie"]
        cookie = cookie[:cookie.index(";")]
        self.set_cookie(cookie)
        self.set_token(token)
        return cookie, token

    def get_status(self, html_and_head):
        """TODO: """
        server_idx = html_and_head.index("Server")
        status_code = html_and_head[:server_idx]
        status_code = status_code[11:14]
        return server_idx, status_code

    def separate(self, html_and_head) -> Tuple[str, Dict[str, str], str]:
        """TODO:"""
        server_idx, status_code = self.get_status(html_and_head)
        doctype_idx = html_and_head.index("<!DOCTYPE")
        header = html_and_head[server_idx:doctype_idx]
        header = header.replace("\n ", " ").splitlines()
        headers = {}
        for line in header:
            split_here = line.find(":")
            headers[line[:split_here]] = line[split_here+2:]
        return status_code, headers, html_and_head[doctype_idx:]

    def post_login(self):
        print("POSTing LOGIN")
        method = "POST"
        version = "HTTP/1.1"
        host = "Host: fakebook.3700.network"
        accept = "Accept: text/html, application/xhtml+xml"
        connnection = "Connection: keep-alive"
        cookie = "Cookie: " + self.cookie
        user_agent = "User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36"
        content_type = "Content-Type: application/x-www-form-urlencoded"
        payload = "username=" + self.username + "&password=" + self.password + "&csrfmiddlewaretoken=" + self.token + "&next=%2Ffakebook%2F"
        content_length = "Content-Length: " + str(len(payload))
        request = method + \
            " " + \
                "/accounts/login/?next=/fakebook/" + \
                    " " + version + \
                        "\r\n" + host + \
                            "\r\n" + content_type + \
                                "\r\n" + connnection + \
                                    "\r\n" + cookie + \
                                    "\r\n" + accept + \
                                    "\r\n" + content_length + \
                                    "\r\n\r\n" + payload 
        print(request)
        s_sock.send(request.encode())
        response = str(s_sock.recv(BUFSIZE))
        print(response)
        _, status = self.get_status(response)
        if (status == "302"):
            location = response[response.index("Location:")+10:response.index("Expires")]
            location = location[:-4]
            self.parse_for_session(response)
            return self.http_get(location)

    def parse_for_session(self, response):
        """Parses the given response string for a session ID."""
        idx = response.index("sessionid=")
        session = response[idx:idx+268]
        self.set_session(session)
    
    def http_get(self, url) -> str:
        """Performs an HTTP GET Request and returns the response
        from Fakebook's server."""
        method = "GET"
        version = "HTTP/1.1"
        host = "Host: fakebook.3700.network"
        accept = "Accept: text/html, application/xhtml+xml"
        connnection = "Connection: keep-alive"
        cookie = "Cookie: " + self.cookie + "; " + self.session
        request = method + \
            " " + \
                url + \
                    " " + version + \
                        "\r\n" + host + \
                                "\r\n" + connnection + \
                                    "\r\n" + cookie + \
                                    "\r\n" + accept + "\r\n\r\n"
        print(request)
        s_sock.send(request.encode())
        response = str(s_sock.recv(BUFSIZE))
        if "sessionid=" in response:
            self.parse_for_session()
        return response, url

    def search_next_page(self):
        url = self.pages_to_visit.pop()
        return self.http_get(url)
        
class AnchorParser(HTMLParser):
    found_urls = []

    def handle_starttag(self, tag, attrs):
        if tag == "a":
            for attr in attrs:
                if attr[0] == "href":
                    self.found_urls.append(attr[1])

crawler = Crawler()
parser = AnchorParser()
if len(sys.argv) != 3:
    print("Please use this module by calling ./webcrawler [username] [password]!")
else:
    crawler.set_username(sys.argv[1])
    crawler.set_password(sys.argv[2])

# scrape for cookie, set to crawler

crawler.get_token_and_cookie()
response, url = crawler.post_login()

# data structure to hold secret flags
secret_flags = []
# TODO: notes
# While loop?
while len(secret_flags) < 5 and crawler.pages_to_visit:
    # get the status code of the response

    if "Connection: close" in response or len(response) == 0:
        # kill socket, start again with new socket
        s_sock.close()
        sock.close()
        context = ssl.create_default_context()
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        s_sock = context.wrap_socket(sock, server_hostname=URL)
        s_sock.connect((URL, PORT))

        response, url = crawler.http_get(url)

    _, status_code = crawler.get_status(response)
    if status_code == "403" or status_code == "404":
        # abandon the current url --> move to the next url in the crawler's deque
        print(status_code)
    elif status_code == "500":
        # try the current request again.
        response, url = crawler.http_get(url)
    elif status_code == "302":
        location = response[response.index("Location:")+10:response.index("Expires")]
        location = location[:-4]
        response, url = crawler.http_get(location)
    elif status_code == "200":
        print("200")
        # look for secret flag
        searchResult = re.findall("secret_flag.+FLAG: ([a-zA-Z0-9]{64})", response)
        if searchResult:
            searchResultGroups = list(map(lambda result : result.group(), searchResult))
            secret_flags.extend(searchResultGroups)
            # TODO: Print secret flags line by line
            print("Secret flag found:", secret_flags)
        
        # look for anchors 
        parser.feed(response)
        new_urls = parser.found_urls
        filtered_urls = list(filter(lambda possible_url : possible_url not in crawler.seen_pages, new_urls))

        crawler.add_urls_to_deque(filtered_urls)
        parser.found_urls = []

        crawler.seen_pages.add(url)

        response, url = crawler.search_next_page()
    else:
        print("Unknown status code:", status_code)
# when len(secret_flags) == 5, exit while loop, sys.exit(0) 
# figure out the "frontier" -> use a deque?
# Chunking? HTTP1.0 vs HTTP1.1
        # if response closing tag != </html>, continue reading from sock
# Need to check for 500s, 400s, 300s, 100s and react accordingly
# #regex search:
# re.search(secret_flag.+FLAG: ([a-zA-Z0-9]{64})) 
